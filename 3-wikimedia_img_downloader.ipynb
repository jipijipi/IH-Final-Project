{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 512\n",
    "\n",
    "API_ENDPOINT = \"https://commons.wikimedia.org/w/api.php\"\n",
    "\n",
    "OUTPUT_DIR = f'img/img_{IMG_WIDTH}'\n",
    "\n",
    "CHECKPOINT_FILE = \"data/checkpoints/download_checkpoint.txt\"\n",
    "\n",
    "MAX_WORKERS = 3 \n",
    "\n",
    "BATCH_SIZE = 30  \n",
    "\n",
    "USER_AGENT = \"IH-final/1.0 (jipijipijipi@gmail.com)\"\n",
    "\n",
    "API_TIMEOUT = 30 \n",
    "DOWNLOAD_TIMEOUT = 60  \n",
    "\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "API_DELAY = 0.5  # seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(url):\n",
    "    parsed = urllib.parse.urlparse(url)\n",
    "    first_decode = urllib.parse.unquote(parsed.path)\n",
    "    filename = os.path.basename(first_decode)\n",
    "    filename = filename.replace(' ', '_')\n",
    "    return filename\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            chunk = [next(it) for _ in range(size)]\n",
    "        except StopIteration:\n",
    "            chunk = []\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "        if len(chunk) < size:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_thumbnail_urls(filenames):\n",
    "\n",
    "    titles = '|'.join([f'File:{filename}' for filename in filenames])\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': titles,\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'url',\n",
    "        'iiurlwidth': f'{IMG_WIDTH}',\n",
    "        'format': 'json',\n",
    "        'formatversion': '2'\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': USER_AGENT\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENDPOINT, params=params, headers=headers, timeout=API_TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        result = {}\n",
    "        pages = data.get('query', {}).get('pages', [])\n",
    "        for page in pages:\n",
    "            title = page.get('title', '')\n",
    "            filename = title.replace('File:', '')\n",
    "            imageinfo = page.get('imageinfo', [])\n",
    "            if imageinfo:\n",
    "                # Prefer 'thumburl' if available, else fallback to 'url'\n",
    "                thumb_url = imageinfo[0].get('thumburl')\n",
    "                if not thumb_url:\n",
    "                    # If thumburl is not available, use the full image URL\n",
    "                    thumb_url = imageinfo[0].get('url')\n",
    "                result[filename] = thumb_url\n",
    "            else:\n",
    "                result[filename] = None\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during fetching thumbnail URLs: {e}\")\n",
    "        return {}\n",
    "\n",
    "def download_image(session, url, filename):\n",
    "\n",
    "    if not url:\n",
    "        print(f\"No URL provided for {filename}. Skipping download.\")\n",
    "        return False\n",
    "\n",
    "    image_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = session.get(url, timeout=DOWNLOAD_TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename} (Attempt {attempt}/{MAX_RETRIES}): {e}\")\n",
    "            time.sleep(1)  # Wait before retrying\n",
    "    print(f\"Failed to download {filename} after {MAX_RETRIES} attempts.\")\n",
    "    return False\n",
    "\n",
    "def update_checkpoint(filename):\n",
    "\n",
    "    with open(CHECKPOINT_FILE, 'a') as f:\n",
    "        f.write(f\"{filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wikidata_all_paintings.csv')\n",
    "#df = pd.read_csv('data/missing_image_paths.csv')\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['image_url'])\n",
    "df['filename'] = df['image_url'].apply(extract_filename)\n",
    "df = df.drop_duplicates(subset=['filename'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "df.dropna(subset=['image_url'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        downloaded = set(line.strip() for line in f)\n",
    "else:\n",
    "    downloaded = set()\n",
    "\n",
    "existing_files = set(os.listdir(OUTPUT_DIR))\n",
    "downloaded.update(existing_files)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['filename'] = df['image_url'].apply(extract_filename)\n",
    "\n",
    "\n",
    "df = df.drop_duplicates(subset='filename')\n",
    "\n",
    "filenames = df['filename'].tolist()\n",
    "filenames_to_download = [fn for fn in filenames if fn.replace('_', ' ') not in downloaded]\n",
    "\n",
    "total_images = len(filenames_to_download)\n",
    "print(f\"Total images to download: {total_images}\")\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({'User-Agent': USER_AGENT})\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    for batch_num, batch in enumerate(chunked_iterable(filenames_to_download, BATCH_SIZE), start=1):\n",
    "        print(f\"Processing batch {batch_num} with {len(batch)} images...\")\n",
    "        thumb_urls = fetch_thumbnail_urls(batch)\n",
    "\n",
    "        tasks = []\n",
    "        for filename, url in thumb_urls.items():\n",
    "            if url:\n",
    "                tasks.append((filename, url))\n",
    "            else:\n",
    "                print(f\"No thumbnail URL found for {filename, url}. Skipping.\")\n",
    "\n",
    "        if not tasks:\n",
    "            print(f\"No downloadable URLs found in batch {batch_num}. Skipping to next batch.\")\n",
    "            continue\n",
    "\n",
    "        with tqdm(total=len(tasks), desc=f\"Batch {batch_num}\", unit=\"image\") as pbar:\n",
    "            future_to_filename = {\n",
    "                executor.submit(download_image, session, url, filename): filename\n",
    "                for filename, url in tasks\n",
    "            }\n",
    "            for future in as_completed(future_to_filename):\n",
    "                filename = future_to_filename[future]\n",
    "                try:\n",
    "                    success = future.result()\n",
    "                    if success:\n",
    "                        update_checkpoint(filename)\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error downloading {filename}: {e}\")\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        time.sleep(API_DELAY)\n",
    "\n",
    "print(\"All downloads completed.\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
