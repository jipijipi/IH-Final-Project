{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import unquote, urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/wikidata_paintings_final_with_wiki_url.csv')\n",
    "#df = paintings[paintings['wikipedia_url'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_wikipedia_article_text(df):\n",
    "    session = requests.Session()\n",
    "    def get_article_text(url):\n",
    "        if pd.isnull(url):\n",
    "            return None\n",
    "        try:\n",
    "            # Parse the URL to extract the article title\n",
    "            path = urlparse(url).path  # e.g., '/wiki/Article_Title'\n",
    "            parts = path.split('/')\n",
    "            if len(parts) < 3 or parts[1] != 'wiki':\n",
    "                return None  # Invalid URL format\n",
    "            title = parts[2]\n",
    "            # Decode any percent-encoded characters\n",
    "            title = unquote(title)\n",
    "            # Prepare the Wikipedia API request\n",
    "            api_url = 'https://en.wikipedia.org/w/api.php'\n",
    "            params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'prop': 'extracts',\n",
    "                'explaintext': True,\n",
    "                'redirects': True,\n",
    "                'titles': title\n",
    "            }\n",
    "            # Make the API request\n",
    "            response = session.get(api_url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            # Extract the page content\n",
    "            pages = data.get('query', {}).get('pages', {})\n",
    "            page = next(iter(pages.values()))\n",
    "            extract = page.get('extract', None)\n",
    "            print(f\"Retrieved article for URL {url}\")\n",
    "            return extract\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving article for URL {url}: {e}\")\n",
    "            return None\n",
    "    # Apply the function to the DataFrame\n",
    "    df['article_text'] = df['wikipedia_url'].apply(get_article_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paintings_with_articles = retrieve_wikipedia_article_text(df)\n",
    "# Step 7: Optional - Save to CSV\n",
    "paintings_with_articles.to_csv('data/wikidata_paintings_final_with_wiki_articles.csv', index=False)\n",
    "\n",
    "print(\"Article retrieval complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paintings_with_articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
