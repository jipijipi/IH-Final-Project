{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"let's go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "from pywikibot import pagegenerators\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import re\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Set columns : AUTHOR TITLE DATE ORIGIN_COUNTRY DISPLAY_COUNTRY DISPLAY_LOCATION TYPE SCHOOL TIMEFRAME? TIME_PERIOD  WA_URL WIKI_URL WIKIDATA_ID IMAGE_URL KEYWORDS\n",
    "2. Get data from web gallery\n",
    "3. Clean and filter data, only paintings, remove dupes, remove (details) cf mona lisa\n",
    "4. Scrape web gallery for description and image url\n",
    "5. Save to CSV, cache\n",
    "6. fetch all images\n",
    "5. Get data from wikidata\n",
    "7. supplement first df\n",
    "6. remove dupes\n",
    "4. fine tune yolo5 for food recognition using wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_wikipedia_abstracts_lxml(xml_file, words_in_abstract):\n",
    "    data = []\n",
    "    context = etree.iterparse(xml_file, events=('end',), tag='doc')\n",
    "    for event, elem in context:\n",
    "        url = elem.findtext('url')\n",
    "        title = elem.findtext('title')\n",
    "        abstract = elem.findtext('abstract')\n",
    "        if abstract:\n",
    "            for word in words_in_abstract:\n",
    "                # Check for exact word match and its plural form\n",
    "                if re.search(r'\\b' + re.escape(word) + r's?\\b', abstract):\n",
    "                    data.append({\n",
    "                        'URL': url,\n",
    "                        'Title': title,\n",
    "                        'Abstract': abstract,\n",
    "                        'Keyword': word\n",
    "                    })\n",
    "                    break\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    return pd.DataFrame(data, columns=['URL', 'Title', 'Abstract', 'Keyword'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your enwiki-latest-abstract.xml file\n",
    "xml_file_path = 'data/enwiki-latest-abstract.xml'\n",
    "\n",
    "words_in_abstract = ['painting', 'gravure', 'illustration', 'mural', 'fresco', 'watercolor', 'pastel', 'etching', 'lithography', 'woodcut', 'silkscreen']\n",
    "\n",
    "# Parse the XML and create the DataFrame\n",
    "wiki_df = parse_wikipedia_abstracts_lxml(xml_file_path, words_in_abstract)\n",
    "\n",
    "# Display the first few rows\n",
    "print(wiki_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "wiki_df.to_csv('data/wikipedia_abstracts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
