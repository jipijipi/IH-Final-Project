{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data\n",
    "df = pd.read_csv('data/paintings_with_filenames.csv') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_relevant_sections(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Define regular expression patterns to match each section\n",
    "    intro_pattern = r'^(.*?)(?=\\n==)'\n",
    "    description_pattern = r'== Description ==\\n(.*?)(?=\\n==|\\Z)'\n",
    "    interpretation_pattern = r'== Interpretations ==\\n(.*?)(?=\\n==|\\Z)'\n",
    "\n",
    "    # Search for each section\n",
    "    intro = re.search(intro_pattern, text, re.DOTALL)\n",
    "    description = re.search(description_pattern, text, re.DOTALL)\n",
    "    interpretation = re.search(interpretation_pattern, text, re.DOTALL)\n",
    "\n",
    "    # Extract the sections, if found\n",
    "    result = \"\"\n",
    "    if intro:\n",
    "        result += intro.group(1).strip() + \"\\n\\n\"\n",
    "    if description:\n",
    "        result += \"== Description ==\\n\" + description.group(1).strip() + \"\\n\\n\"\n",
    "    if interpretation:\n",
    "        result += \"== Subject ==\\n\" + interpretation.group(1).strip()\n",
    "\n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['wiki_description'] = df['article_text'].apply(extract_relevant_sections)\n",
    "\n",
    "df['full_description'] = df['title'] + ' ' + df['depicts'] + ' ' + df['wga_description'] + ' ' + df['wiki_description']\n",
    "\n",
    "# Filter out NaN values from 'full_description'\n",
    "df = df.dropna(subset=['full_description'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "#df = df[df['full_description'].str.contains('supper', case=False, na=False)]\n",
    "df = df.head(128)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the minimum confidence score threshold\n",
    "score_threshold = 0.5\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"chambliss/distilbert-for-food-extraction\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Print available labels in the model\n",
    "labels = model.config.id2label\n",
    "print(\"Available labels:\", labels)\n",
    "\n",
    "# Define NER pipeline with batch processing\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if device.type == \"mps\" else -1  # 0 for GPU/MPS, -1 for CPU\n",
    ")\n",
    "\n",
    "# Sample dataframe with descriptions\n",
    "df_test = pd.DataFrame({\n",
    "    'full_description': [\n",
    "        \"There are herring busses in front of the Rotterdam Gate.\",\n",
    "        \"People are eating fruits.\",\n",
    "        \"People are eating apples.\",\n",
    "        \"People are eating dirt.\",\n",
    "        \"People are feasting wines.\",\n",
    "        \"The painting shows a cityscape with a harbor.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Function to organize entities by label, filtering by score threshold\n",
    "def organize_entities_by_label(results_batch, score_threshold):\n",
    "    organized_entities_list = []\n",
    "\n",
    "    # Process each description in the batch\n",
    "    for results in results_batch:\n",
    "        entity_dict = {label: [] for label in labels.values()}\n",
    "        for entity in results:\n",
    "            if entity['score'] >= score_threshold:  # Only consider entities above the score threshold\n",
    "                label = labels[int(entity['entity_group'][-1])]  # Convert entity_group like \"LABEL_0\" to int and get label\n",
    "                entity_dict[label].append(entity['word'])\n",
    "        organized_entities_list.append(entity_dict)\n",
    "    \n",
    "    return organized_entities_list\n",
    "\n",
    "# Process descriptions in batches and organize entities by label\n",
    "batch_size = 16\n",
    "descriptions = df['full_description'].tolist()\n",
    "all_organized_entities = []\n",
    "\n",
    "num_batches = (len(descriptions) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in tqdm(range(0, len(descriptions), batch_size), total=num_batches, desc=\"Processing Batches\", unit=\"batch\", ncols=80, leave=True):\n",
    "    batch_texts = descriptions[i:i + batch_size]\n",
    "    results_batch = ner_pipeline(batch_texts)\n",
    "    \n",
    "    # Organize entities by label for each batch with score filtering\n",
    "    all_organized_entities.extend(organize_entities_by_label(results_batch, score_threshold))\n",
    "\n",
    "# Convert organized entities to separate columns in the DataFrame\n",
    "for label in labels.values():\n",
    "    df[label] = [entities[label] for entities in all_organized_entities]\n",
    "\n",
    "# Display the updated dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"chambliss/distilbert-for-food-extraction\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Print available labels in the model\n",
    "labels = model.config.id2label\n",
    "print(\"Available labels:\", labels)\n",
    "\n",
    "\n",
    "# Define NER pipeline with batch processing\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if device.type == \"mps\" else -1  # 0 for GPU/MPS, -1 for CPU\n",
    ")\n",
    "\n",
    "# Sample dataframe with descriptions\n",
    "df_test = pd.DataFrame({\n",
    "    'full_description': [\n",
    "        \"There are herring busses in front of the Rotterdam Gate.\",\n",
    "        \"People are eating fruit.\",\n",
    "        \"People are eating apples.\",\n",
    "        \"People are eating so many apples.\",\n",
    "        \"People are eating so many biscuits.\",\n",
    "        \"People are moving apples.\",\n",
    "        \"People are eating dirt.\",\n",
    "        \"People are feasting wines.\",\n",
    "        \"The painting shows a cityscape with a harbor.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Function to organize entities by label\n",
    "def organize_entities_by_label(results_batch):\n",
    "    # Prepare a dictionary to store entities by label for each description\n",
    "    organized_entities = {label: [] for label in labels.values()}\n",
    "    organized_entities_list = []\n",
    "\n",
    "    # Process each description in the batch\n",
    "    for results in results_batch:\n",
    "        print(results)\n",
    "        entity_dict = {label: [] for label in labels.values()}\n",
    "        for entity in results:\n",
    "            label = labels[int(entity['entity_group'][-1])]  # Convert entity_group like \"LABEL_0\" to int and get label\n",
    "            entity_dict[label].append(entity['word'])\n",
    "        organized_entities_list.append(entity_dict)\n",
    "    \n",
    "    return organized_entities_list\n",
    "\n",
    "# Define set of food entities for faster lookup\n",
    "food_entities = {'food','wine', 'beverage', 'meat', 'fruit', 'vegetable', 'bread', \n",
    "                'dairy', 'dessert', 'seafood', 'eggs', 'fish'}\n",
    "\n",
    "# Process descriptions in batches and organize entities by label\n",
    "batch_size = 16\n",
    "descriptions = df['full_description'].tolist()\n",
    "all_organized_entities = []\n",
    "\n",
    "num_batches = (len(descriptions) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in tqdm(range(0, len(descriptions), batch_size), total=num_batches, desc=\"Processing Batches\", unit=\"batch\", ncols=80, leave=True):\n",
    "    batch_texts = descriptions[i:i + batch_size]\n",
    "    results_batch = ner_pipeline(batch_texts)\n",
    "    \n",
    "    # Organize entities by label for each batch\n",
    "    all_organized_entities.extend(organize_entities_by_label(results_batch))\n",
    "\n",
    "# Convert organized entities to separate columns in the DataFrame\n",
    "for label in labels.values():\n",
    "    df[label] = [entities[label] for entities in all_organized_entities]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/mentions_food.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
