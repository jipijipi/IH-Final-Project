{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO import from web https://www.wga.hu/index_database.html\n",
    "#TODO setup caching system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artworks = pd.read_csv('data/catalog.txt', delimiter='\\t', quotechar='\"', encoding='iso-8859-1')\n",
    "all_paintings = pd.read_csv('data/wikidata_paintings_final_with_wiki_articles.csv')\n",
    "display(artworks)\n",
    "display(all_paintings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(artworks['FORM'].value_counts())\n",
    "\n",
    "wga_paintings = artworks[artworks['FORM'] == 'painting']\n",
    "wga_paintings.rename(columns={'URL': 'wga_url'}, inplace=True)\n",
    "wga_paintings.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge url to df with wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and standardize author names\n",
    "def clean_author_name(name):\n",
    "    # Extract the part before the tab character\n",
    "    name = name.split('\\t')[0]\n",
    "    # Split into last name and first name\n",
    "    if ',' in name:\n",
    "        last, first = name.split(',', 1)\n",
    "        name = first.strip() + ' ' + last.strip()\n",
    "    else:\n",
    "        name = name.strip()\n",
    "    # Remove extra spaces and convert to title case\n",
    "    name = ' '.join(name.split()).title()\n",
    "    return name\n",
    "\n",
    "\n",
    "# Apply the function to df2\n",
    "wga_paintings['author_name'] = wga_paintings['AUTHOR'].apply(clean_author_name)\n",
    "\n",
    "# Clean the 'author_name' in df1\n",
    "all_paintings['author_name'] = all_paintings['author_name'].apply(lambda x: ' '.join(x.split()).title())\n",
    "\n",
    "# Clean and standardize the titles in both dataframes\n",
    "def clean_title(title):\n",
    "    title = title.strip().lower()\n",
    "    # Remove special characters\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    return title\n",
    "\n",
    "all_paintings['title_clean'] = all_paintings['title'].apply(clean_title)\n",
    "wga_paintings['title_clean'] = wga_paintings['TITLE'].apply(clean_title)\n",
    "\n",
    "# Merge the dataframes on 'author_name' and 'title_clean'\n",
    "all_paintings = pd.merge(all_paintings, wga_paintings[['author_name','title_clean','wga_url']], on=['author_name', 'title_clean'], how='left')\n",
    "\n",
    "# Display the merged dataframe\n",
    "display(all_paintings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get description from WGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='wga_scraping.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create a cache directory\n",
    "cache_dir = 'data/wga_cache'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def get_cached_description(wga_url):\n",
    "    cache_file = os.path.join(cache_dir, f\"{hash(wga_url)}.json\")\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('description')\n",
    "    return None\n",
    "\n",
    "def cache_description(wga_url, description):\n",
    "    cache_file = os.path.join(cache_dir, f\"{hash(wga_url)}.json\")\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'description': description}, f)\n",
    "\n",
    "def scrape_wga_description_direct(session, wga_url):\n",
    "    \"\"\"\n",
    "    Scrapes the description of an artwork directly from the wga_url.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check cache first\n",
    "        cached_description = get_cached_description(wga_url)\n",
    "        if cached_description:\n",
    "            logging.info(f\"Description retrieved from cache for URL: {wga_url}\")\n",
    "            return cached_description\n",
    "\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; YourBot/0.1; +http://yourwebsite.com/bot)'\n",
    "        }\n",
    "        response = session.get(wga_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Locate the <p> tag containing the description\n",
    "        # Based on your provided HTML, the description is within a <p> tag inside a <td>\n",
    "        # Adjust the selectors accordingly\n",
    "\n",
    "        # Locate the <h3> tag with the artwork title\n",
    "        title_tag = soup.find('h3')\n",
    "        if title_tag:\n",
    "            parent_td = title_tag.find_parent('td')\n",
    "            if parent_td:\n",
    "                # Find all <p> tags within this <td>\n",
    "                p_tags = parent_td.find_all('p')\n",
    "                for p in p_tags:\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if len(text) > 50:  # Adjust the length threshold as needed\n",
    "                        cache_description(wga_url, text)\n",
    "                        return text\n",
    "\n",
    "        # Fallback: find the first <p> tag with sufficient text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if len(text) > 50:\n",
    "                cache_description(wga_url, text)\n",
    "                return text\n",
    "\n",
    "        # If no suitable description is found\n",
    "        logging.warning(f\"Description not found in URL: {wga_url}\")\n",
    "        return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request error for URL {wga_url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing URL {wga_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a session with retries\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "\n",
    "\n",
    "wga_df = all_paintings[all_paintings['wga_url'].notna() & (all_paintings['wga_url'].str.strip() != '')].copy()\n",
    "wga_df.reset_index(inplace=True)\n",
    "\n",
    "# Initialize a list to store descriptions\n",
    "descriptions = []\n",
    "\n",
    "for idx, row in wga_df.iterrows():\n",
    "    original_wga_url = row['wga_url']\n",
    "    logging.info(f\"Processing {idx + 1}/{len(wga_df)}: {original_wga_url}\")\n",
    "\n",
    "    # Directly scrape the description from the wga_url\n",
    "    description = scrape_wga_description_direct(session, original_wga_url)\n",
    "    descriptions.append(description)\n",
    "\n",
    "    # Respectful delay to avoid overwhelming the server\n",
    "    time.sleep(1)  # Adjust the delay as needed\n",
    "\n",
    "# Add the descriptions to the dataframe\n",
    "wga_df['wga_description'] = descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wga_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge back to the original dataframe using the original index\n",
    "all_paintings = all_paintings.merge(wga_df[['wga_url', 'wga_description']], on='wga_url', how='left')\n",
    "\n",
    "\n",
    "# Save the updated dataframe\n",
    "all_paintings.to_csv('data/paintings_with_descriptions.csv', index=False)\n",
    "\n",
    "# Optional: Print a success message\n",
    "print(\"Scraping completed. Descriptions added to 'paintings_with_descriptions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
