{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import re\n",
    "from urllib.parse import unquote, quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize global session for connection pooling\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'ArtDataBot/1.0 (jipijipijipi@gmail.com) Python/requests',\n",
    "    'Accept': 'application/json'\n",
    "})\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)  # Limit to 5 calls per second\n",
    "def run_sparql_query(query: str, endpoint_url: str) -> Dict:\n",
    "    \"\"\"Execute SPARQL query with rate limiting\"\"\"\n",
    "    response = session.get(\n",
    "        endpoint_url,\n",
    "        params={'query': query, 'format': 'json'},\n",
    "        timeout=30\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def process_batch(batch_qids: List[str], endpoint_url: str, query_template: str, src_column_name: str, max_retries: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Process a single batch of QIDs with a customizable SPARQL query template.\"\"\"\n",
    "    qid_list_str = ' '.join(f'wd:{qid}' for qid in batch_qids)\n",
    "    # Insert the QID list and src_column_name into the query template\n",
    "    batch_query = query_template.format(qid_list=qid_list_str, src_column_name=src_column_name)\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            results = run_sparql_query(batch_query, endpoint_url)\n",
    "            bindings = results['results']['bindings']\n",
    "            \n",
    "            if not bindings:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Dynamically construct data extraction based on available fields\n",
    "            data = []\n",
    "            for b in bindings:\n",
    "                row = {key: b.get(key, {}).get('value') for key in b}\n",
    "                data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if retry == max_retries - 1:\n",
    "                print(f\"Max retries exceeded: {e}\")\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(2 ** retry)  # Exponential backoff\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "def chunk_list(lst: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"Split a list into chunks of specified size\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def get_supplement_from_wikidata(name, src_path, src_column_name, query_template: str):\n",
    "    # Configuration\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    batch_size = 50\n",
    "    max_workers = 3  # Adjust based on your needs and API limits\n",
    "    checkpoint_frequency = 10  # Save checkpoint every N batches\n",
    "\n",
    "    if os.path.exists(f'data/wikidata_{name}.csv'):\n",
    "        print(\"Final file already exists. Skipping data retrieval.\")\n",
    "        return\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs('data/checkpoints', exist_ok=True)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    checkpoint_file = f'data/checkpoints/{name}_checkpoint.csv'\n",
    "    batch_index_file = f'data/checkpoints/{name}_batch_index_checkpoint.txt'\n",
    "    \n",
    "    if os.path.exists(checkpoint_file) and os.path.exists(batch_index_file):\n",
    "        detailed_data = pd.read_csv(checkpoint_file)\n",
    "        with open(batch_index_file, 'r') as f:\n",
    "            start_batch = int(f.read())\n",
    "        print(f\"Resuming from batch {start_batch}\")\n",
    "    else:\n",
    "        detailed_data = pd.DataFrame()\n",
    "        start_batch = 0\n",
    "\n",
    "    # Read and prepare author data\n",
    "    basic_data = pd.read_csv(src_path)\n",
    "    results = basic_data[[src_column_name]].drop_duplicates().reset_index(drop=True)\n",
    "    results = results.dropna(subset=[src_column_name])  # Drop rows where 'author_wikidata' is NaN\n",
    "    # Drop values that do not start with 'http://www.wikidata.org/entity'\n",
    "    results = results[results[src_column_name].str.startswith('http://www.wikidata.org/entity')]\n",
    "    # Convert item URIs to Q-IDs and create batches\n",
    "    src_qids = [uri.split('/')[-1] for uri in results[src_column_name].tolist()]\n",
    "    \n",
    "    batches = chunk_list(src_qids, batch_size)\n",
    "\n",
    "    print(f\"Processing {len(batches)} batches with {len(src_qids)} records...\")\n",
    "\n",
    "    # Process batches with ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_batch, batch, endpoint_url, query_template, src_column_name): batch_idx \n",
    "            for batch_idx, batch in enumerate(batches[start_batch:], start=start_batch)\n",
    "        }\n",
    "        \n",
    "        completed_batches = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            batch_idx = futures[future]\n",
    "            try:\n",
    "                batch_df = future.result()\n",
    "                if not batch_df.empty:\n",
    "                    detailed_data = pd.concat([detailed_data, batch_df], ignore_index=True)\n",
    "                \n",
    "                completed_batches += 1\n",
    "                print(f\"Completed batch {batch_idx + 1}/{len(batches)} \"\n",
    "                      f\"({(batch_idx + 1)/len(batches)*100:.1f}%)\")\n",
    "\n",
    "                # Save checkpoint periodically\n",
    "                if completed_batches % checkpoint_frequency == 0:\n",
    "                    detailed_data.to_csv(checkpoint_file, index=False)\n",
    "                    with open(batch_index_file, 'w') as f:\n",
    "                        f.write(str(batch_idx + 1))\n",
    "                    print(f\"Checkpoint saved at batch {batch_idx + 1}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "\n",
    "    # Final processing\n",
    "    print(\"Processing complete. Preparing final dataset...\")\n",
    "    detailed_data.drop_duplicates(inplace=True)\n",
    "    final_data = pd.merge(results, detailed_data, on=src_column_name, how='left')\n",
    "    \n",
    "    # Save final results\n",
    "    final_data.to_csv(f'data/wikidata_{name}.csv', index=False)\n",
    "    print(f\"Data saved to wikidata_{name}.csv\")\n",
    "\n",
    "    # Clean up checkpoints\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "    if os.path.exists(batch_index_file):\n",
    "        os.remove(batch_index_file)\n",
    "\n",
    "\n",
    "author_query_template = \"\"\"\n",
    "SELECT DISTINCT ?{src_column_name} ?author_name ?country ?country_label ?gender ?gender_label \n",
    "       ?date_of_birth ?place_of_birth ?place_of_birth_label \n",
    "       ?place_of_birth_country ?place_of_birth_country_label \n",
    "WHERE {{\n",
    "    VALUES ?{src_column_name} {{ {qid_list} }}\n",
    "    \n",
    "    # Author-specific information here\n",
    "    OPTIONAL {{\n",
    "        ?{src_column_name} rdfs:label ?author_name.\n",
    "        FILTER(LANG(?author_name) = \"en\")\n",
    "    }}\n",
    "    OPTIONAL {{\n",
    "        ?{src_column_name} wdt:P27 ?country.\n",
    "        ?country rdfs:label ?country_label.\n",
    "        FILTER(LANG(?country_label) = \"en\")\n",
    "    }}\n",
    "    OPTIONAL {{\n",
    "        ?{src_column_name} wdt:P21 ?gender.\n",
    "        ?gender rdfs:label ?gender_label.\n",
    "        FILTER(LANG(?gender_label) = \"en\")\n",
    "    }}\n",
    "    OPTIONAL {{\n",
    "        ?{src_column_name} wdt:P569 ?date_of_birth.\n",
    "    }}\n",
    "    OPTIONAL {{\n",
    "        ?{src_column_name} wdt:P19 ?place_of_birth.\n",
    "        ?place_of_birth rdfs:label ?place_of_birth_label.\n",
    "        FILTER(LANG(?place_of_birth_label) = \"en\")\n",
    "        OPTIONAL {{\n",
    "            ?place_of_birth wdt:P17 ?place_of_birth_country.\n",
    "            ?place_of_birth_country rdfs:label ?place_of_birth_country_label.\n",
    "            FILTER(LANG(?place_of_birth_country_label) = \"en\")\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "paintings_query_template = \"\"\"\n",
    "    SELECT ?item ?title ?creation_date ?origin_country ?display_country ?type ?school ?time_period ?image_url (GROUP_CONCAT(?depicts_label; separator=\", \") AS ?depicts) WHERE {{\n",
    "    VALUES ?item {{ {qid_list} }}\n",
    "\n",
    "    # Title of the item\n",
    "    OPTIONAL {{\n",
    "        ?item rdfs:label ?title.\n",
    "        FILTER(LANG(?title) = \"en\")\n",
    "    }}\n",
    "    \n",
    "    OPTIONAL {{ ?item wdt:P571 ?creation_date. }}\n",
    "    OPTIONAL {{ ?item wdt:P571 ?creation_date. }}\n",
    "    \n",
    "    # Origin country of the item or the author\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P495 ?origin_country_wd.\n",
    "        ?origin_country_wd rdfs:label ?origin_country.\n",
    "        FILTER(LANG(?origin_country) = \"en\")\n",
    "    }}\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P50 ?author.\n",
    "        ?author wdt:P27 ?author_country_wd.\n",
    "        ?author_country_wd rdfs:label ?author_country.\n",
    "        FILTER(LANG(?author_country) = \"en\")\n",
    "    }}\n",
    "    BIND(COALESCE(?origin_country, ?author_country) AS ?origin_country)\n",
    "    \n",
    "    # Display country\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P276 ?display_location_wd.\n",
    "        OPTIONAL {{\n",
    "            ?display_location_wd wdt:P17 ?display_country_wd.\n",
    "            ?display_country_wd rdfs:label ?display_country.\n",
    "            FILTER(LANG(?display_country) = \"en\")\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    # Type\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P136 ?type_wd.\n",
    "        ?type_wd rdfs:label ?type.\n",
    "        FILTER(LANG(?type) = \"en\")\n",
    "    }}\n",
    "    \n",
    "    # School or tradition\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P135 ?school_wd.\n",
    "        ?school_wd rdfs:label ?school.\n",
    "        FILTER(LANG(?school) = \"en\")\n",
    "    }}\n",
    "    \n",
    "    # Time period\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P2348 ?time_period_wd.\n",
    "        ?time_period_wd rdfs:label ?time_period.\n",
    "        FILTER(LANG(?time_period) = \"en\")\n",
    "    }}\n",
    "    \n",
    "    # Image URL\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P18 ?image_url.\n",
    "    }}\n",
    "    \n",
    "    # Depicts\n",
    "    OPTIONAL {{\n",
    "        ?item wdt:P180 ?depicts_wd.\n",
    "        ?depicts_wd rdfs:label ?depicts_label.\n",
    "        FILTER(LANG(?depicts_label) = \"en\")\n",
    "    }}\n",
    "    }}\n",
    "    GROUP BY ?item ?title ?creation_date ?origin_country ?display_country ?type ?school ?time_period ?image_url\n",
    "\"\"\"\n",
    "\n",
    "locations_query_template = \"\"\"\n",
    "    SELECT ?{src_column_name} ?museum_name ?city ?city_label ?country ?country_label \n",
    "           ?founding_date ?museum_type ?museum_type_label ?coordinates ?part_of \n",
    "    WHERE {{\n",
    "        VALUES ?{src_column_name} {{ {qid_list} }}\n",
    "\n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P17 ?country.                             # P17 = country\n",
    "            ?country rdfs:label ?country_label.                              # Get the label for country\n",
    "            FILTER(LANG(?country_label) = \"en\")\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P131 ?city.                               # P131 = city\n",
    "            ?city rdfs:label ?city_label.                                    # Get the label for city\n",
    "            FILTER(LANG(?city_label) = \"en\")\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P571 ?founding_date.                      # P571 = founding date\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P31 ?museum_type.                         # P31 = instance of (museum type)\n",
    "            ?museum_type rdfs:label ?museum_type_label.                      # Get the label for museum type\n",
    "            FILTER(LANG(?museum_type_label) = \"en\")\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P625 ?coordinates.                        # P625 = coordinates (retrieves Geo-coordinates)\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} rdfs:label ?museum_name.                      # Get the museum's name\n",
    "            FILTER(LANG(?museum_name) = \"en\")\n",
    "        }}\n",
    "        \n",
    "        OPTIONAL {{\n",
    "            ?{src_column_name} wdt:P361 ?part_of.                            # P361 = part of (retrieves parent entities)\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "      \n",
    "get_supplement_from_wikidata('all_painters', 'data/wikidata_paintings_ids_final_2.csv', 'author_wikidata', author_query_template,)\n",
    "get_supplement_from_wikidata('all_paintings', 'data/wikidata_paintings_ids_final_2.csv', 'item', paintings_query_template,)\n",
    "get_supplement_from_wikidata('all_locations', 'data/wikidata_paintings_ids_final_2.csv', 'location_wikidata', locations_query_template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_to_thumbnail(url, size=512):\n",
    "    \"\"\"\n",
    "    Convert a Wikimedia Commons URL to its thumbnail version\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): Original Wikimedia Commons URL\n",
    "    size (int): Desired thumbnail width in pixels\n",
    "    \n",
    "    Returns:\n",
    "    str: Thumbnail URL\n",
    "    \"\"\"\n",
    "    # Handle empty or invalid URLs\n",
    "    if not isinstance(url, str) or not url:\n",
    "        return url\n",
    "        \n",
    "    # Extract filename from different possible URL formats\n",
    "    patterns = [\n",
    "        r'commons\\.wikimedia\\.org/wiki/File:(.+?)(?:\\?|$)',  # Wiki page URL\n",
    "        r'upload\\.wikimedia\\.org/wikipedia/commons/[a-f0-9]/[a-f0-9]{2}/(.+?)(?:\\?|$)',  # Direct file URL\n",
    "        r'commons\\.wikimedia\\.org/w/index\\.php\\?title=File:(.+?)&'  # PHP page URL\n",
    "    ]\n",
    "    \n",
    "    filename = None\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            break\n",
    "    \n",
    "    if not filename:\n",
    "        return url\n",
    "        \n",
    "    # URL decode the filename\n",
    "    filename = unquote(filename)\n",
    "    \n",
    "    # Handle special characters in filename\n",
    "    filename = quote(filename)\n",
    "    \n",
    "    # MD5 hash first character and first two characters\n",
    "    import hashlib\n",
    "    md5 = hashlib.md5(filename.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Construct thumbnail URL\n",
    "    thumb_url = f\"https://upload.wikimedia.org/wikipedia/commons/thumb/{md5[0]}/{md5[0:2]}/{filename}/{size}px-{filename}\"\n",
    "    \n",
    "    # Special handling for SVG files\n",
    "    if filename.lower().endswith('.svg'):\n",
    "        thumb_url += '.png'\n",
    "    \n",
    "    return thumb_url\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "def convert_df_urls(df, column_name, size=512):\n",
    "    \"\"\"\n",
    "    Convert all URLs in a DataFrame column to thumbnail URLs\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    column_name (str): Name of the column containing URLs\n",
    "    size (int): Desired thumbnail width in pixels\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with converted URLs\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[column_name] = df[column_name].apply(lambda x: convert_to_thumbnail(x, size))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
