{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the IDs from the SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run the SPARQL query\n",
    "def run_sparql_query(query):\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    # Set your user agent to comply with Wikidata's policy\n",
    "    sparql.addCustomHttpHeader('User-Agent', 'MyPaintingDataRetriever/1.0 (jipijipijipi@gmail.com)')\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "# Function to chunk the list into batches\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "# Skip if the final file already exists\n",
    "if os.path.exists('data/wikidata_paintings_ids.csv'):\n",
    "    print(\"Final file already exists. Skipping data retrieval.\")\n",
    "else:\n",
    "    wikidata_base_query = \"\"\"\n",
    "    SELECT ?item ?title ?author_wikidata ?author_name WHERE {{\n",
    "    ?item wdt:P31 wd:Q3305213.\n",
    "    ?item rdfs:label ?title.\n",
    "    ?item wdt:P170 ?author_wikidata.\n",
    "    ?author_wikidata rdfs:label ?author_name.\n",
    "    FILTER(LANG(?title) = \"en\").\n",
    "    FILTER(LANG(?author_name) = \"en\").\n",
    "    }}\n",
    "    LIMIT {limit}\n",
    "    OFFSET {offset}\n",
    "    \"\"\"\n",
    "\n",
    "    # Set batch parameters\n",
    "    limit = 1000  # Number of records to fetch per batch\n",
    "    checkpoint_interval = 10  # Save a checkpoint every 10 batches\n",
    "    max_retries = 5  # Maximum number of retries for failed requests\n",
    "\n",
    "    # Check if a checkpoint exists to resume from\n",
    "    # Create checkpoints folder if it does not exist\n",
    "    if not os.path.exists('data/checkpoints'):\n",
    "        os.makedirs('data/checkpoints')\n",
    "    if os.path.exists('data/checkpoints/paintings_checkpoint.csv') and os.path.exists('data/checkpoints/offset_checkpoint.txt'):\n",
    "        all_data = pd.read_csv('data/checkpoints/paintings_checkpoint.csv')\n",
    "        with open('data/checkpoints/offset_checkpoint.txt', 'r') as f:\n",
    "            offset = int(f.read())\n",
    "        batch_number = offset // limit\n",
    "        print(f\"Resuming from offset {offset}\")\n",
    "    else:\n",
    "        all_data = pd.DataFrame()\n",
    "        offset = 0\n",
    "        batch_number = 0\n",
    "\n",
    "# Loop to fetch data in batches\n",
    "\n",
    "    while True:\n",
    "        query = wikidata_base_query.format(limit=limit, offset=offset)\n",
    "        print(f\"Fetching data with OFFSET {offset}\")\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                results = run_sparql_query(query)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying ({retries+1}/{max_retries})...\")\n",
    "                retries += 1\n",
    "                time.sleep(5)\n",
    "        else:\n",
    "            print(\"Max retries exceeded. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Process the results\n",
    "        bindings = results['results']['bindings']\n",
    "        if not bindings:\n",
    "            print(\"No more data returned.\")\n",
    "            break\n",
    "\n",
    "        # Convert the bindings to a DataFrame\n",
    "        data = []\n",
    "        for b in bindings:\n",
    "            item = b['item']['value']\n",
    "            title = b['title']['value']\n",
    "            author_wikidata = b['author_wikidata']['value']\n",
    "            author_name = b['author_name']['value']\n",
    "            data.append({\n",
    "                'item': item,\n",
    "                'title': title,\n",
    "                'author_wikidata': author_wikidata,\n",
    "                'author_name': author_name\n",
    "            })\n",
    "        df = pd.DataFrame(data)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "        # Save a checkpoint at specified intervals\n",
    "        batch_number += 1\n",
    "        if batch_number % checkpoint_interval == 0:\n",
    "            all_data.to_csv('data/checkpoints/paintings_checkpoint.csv', index=False)\n",
    "            with open('data/checkpoints/offset_checkpoint.txt', 'w') as f:\n",
    "                f.write(str(offset + limit))\n",
    "            print(f\"Checkpoint saved at batch {batch_number}\")\n",
    "\n",
    "        # Update the offset for the next batch\n",
    "        offset += limit\n",
    "        time.sleep(1)  # Be polite and avoid overloading the server\n",
    "\n",
    "    # Save the final data to a CSV file\n",
    "    all_data.to_csv('data/wikidata_paintings_ids.csv', index=False)\n",
    "    print(\"Data retrieval complete. Saved to wikidata_paintings_ids.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all the data from the previous list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SPARQL endpoint URL\n",
    "wikidata_endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# Read the basic painting data\n",
    "basic_data = pd.read_csv('data/wikidata_paintings_ids.csv')\n",
    "item_list = basic_data['item'].tolist()\n",
    "\n",
    "# Set batch parameters\n",
    "batch_size = 50  # Number of paintings to query at once\n",
    "max_retries = 5  # Maximum number of retries for failed requests\n",
    "\n",
    "# Prepare to store detailed data\n",
    "detailed_data = pd.DataFrame()\n",
    "\n",
    "# Check if a checkpoint exists to resume from\n",
    "if os.path.exists('data/checkpoints/paintings_detailed_checkpoint.csv') and os.path.exists('data/checkpoints/batch_index_checkpoint.txt'):\n",
    "    detailed_data = pd.read_csv('data/checkpoints/paintings_detailed_checkpoint.csv')\n",
    "    with open('data/checkpoints/batch_index_checkpoint.txt', 'r') as f:\n",
    "        start_batch = int(f.read())\n",
    "    print(f\"Resuming from batch index {start_batch}\")\n",
    "else:\n",
    "    detailed_data = pd.DataFrame()\n",
    "    start_batch = 0\n",
    "\n",
    "# Convert item URIs to Q-IDs\n",
    "item_qids = [uri.split('/')[-1] for uri in item_list]\n",
    "\n",
    "# Create batches\n",
    "batches = list(chunk_list(item_qids, batch_size))\n",
    "\n",
    "# Loop over batches to fetch detailed data\n",
    "for batch_index, batch_qids in enumerate(batches[start_batch:], start=start_batch):\n",
    "    print(f\"Processing batch {batch_index + 1}/{len(batches)}\")\n",
    "    qid_list_str = ' '.join(f'wd:{qid}' for qid in batch_qids)\n",
    "\n",
    "    # Construct the SPARQL query for the batch\n",
    "    batch_query = f\"\"\"\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    SELECT ?item ?creation_date ?origin_country ?display_country ?display_location ?type ?school ?time_period ?wiki_url ?image_url (GROUP_CONCAT(?depicts_label; separator=\", \") AS ?depicts) WHERE {{\n",
    "      VALUES ?item {{ {qid_list_str} }}\n",
    "      \n",
    "      OPTIONAL {{ ?item wdt:P571 ?creation_date. }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P495 ?origin_country_wd.\n",
    "        ?origin_country_wd rdfs:label ?origin_country.\n",
    "        FILTER(LANG(?origin_country) = \"en\")\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P276 ?display_location_wd.\n",
    "        ?display_location_wd rdfs:label ?display_location.\n",
    "        FILTER(LANG(?display_location) = \"en\")\n",
    "        OPTIONAL {{\n",
    "          ?display_location_wd wdt:P17 ?display_country_wd.\n",
    "          ?display_country_wd rdfs:label ?display_country.\n",
    "          FILTER(LANG(?display_country) = \"en\")\n",
    "        }}\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P136 ?type_wd.\n",
    "        ?type_wd rdfs:label ?type.\n",
    "        FILTER(LANG(?type) = \"en\")\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P135 ?school_wd.\n",
    "        ?school_wd rdfs:label ?school.\n",
    "        FILTER(LANG(?school) = \"en\")\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P2348 ?time_period_wd.\n",
    "        ?time_period_wd rdfs:label ?time_period.\n",
    "        FILTER(LANG(?time_period) = \"en\")\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P18 ?image_file.\n",
    "        BIND(CONCAT(\"https://commons.wikimedia.org/wiki/Special:FilePath/\", ENCODE_FOR_URI(REPLACE(STR(?image_file), \"^.*\\\\\\\\/(?!.*\\\\\\\\/)\", \"\"))) AS ?image_url)\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?item wdt:P180 ?depicts_wd.\n",
    "        ?depicts_wd rdfs:label ?depicts_label.\n",
    "        FILTER(LANG(?depicts_label) = \"en\")\n",
    "      }}\n",
    "      OPTIONAL {{\n",
    "        ?sitelink schema:about ?item;\n",
    "                  schema:isPartOf <https://en.wikipedia.org/>;\n",
    "                  schema:url ?wiki_url.\n",
    "      }}\n",
    "    }}\n",
    "    GROUP BY ?item ?creation_date ?origin_country ?display_country ?display_location ?type ?school ?time_period ?wiki_url ?image_url\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            results = run_sparql_query(batch_query)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print(\"Max retries exceeded for this batch. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Process the results\n",
    "    bindings = results['results']['bindings']\n",
    "    if not bindings:\n",
    "        print(f\"No data returned for batch {batch_index + 1}.\")\n",
    "        continue\n",
    "\n",
    "    # Convert the bindings to a DataFrame\n",
    "    data = []\n",
    "    for b in bindings:\n",
    "        item = b['item']['value']\n",
    "        creation_date = b.get('creation_date', {}).get('value', None)\n",
    "        origin_country = b.get('origin_country', {}).get('value', None)\n",
    "        display_country = b.get('display_country', {}).get('value', None)\n",
    "        display_location = b.get('display_location', {}).get('value', None)\n",
    "        type_ = b.get('type', {}).get('value', None)\n",
    "        school = b.get('school', {}).get('value', None)\n",
    "        time_period = b.get('time_period', {}).get('value', None)\n",
    "        wiki_url = b.get('wiki_url', {}).get('value', None)\n",
    "        image_url = b.get('image_url', {}).get('value', None)\n",
    "        depicts = b.get('depicts', {}).get('value', None)\n",
    "        data.append({\n",
    "            'item': item,\n",
    "            'creation_date': creation_date,\n",
    "            'origin_country': origin_country,\n",
    "            'display_country': display_country,\n",
    "            'display_location': display_location,\n",
    "            'type': type_,\n",
    "            'school': school,\n",
    "            'time_period': time_period,\n",
    "            'wiki_url': wiki_url,\n",
    "            'image_url': image_url,\n",
    "            'depicts': depicts\n",
    "        })\n",
    "    df = pd.DataFrame(data)\n",
    "    detailed_data = pd.concat([detailed_data, df], ignore_index=True)\n",
    "\n",
    "    # Save a checkpoint\n",
    "    detailed_data.to_csv('data/checkpoints/paintings_detailed_checkpoint.csv', index=False)\n",
    "    with open('data/checkpoints/batch_index_checkpoint.txt', 'w') as f:\n",
    "        f.write(str(batch_index + 1))\n",
    "    print(f\"Checkpoint saved at batch {batch_index + 1}\")\n",
    "\n",
    "    time.sleep(1)  # Be polite and avoid overloading the server\n",
    "\n",
    "# Merge basic and detailed data\n",
    "final_data = pd.merge(basic_data, detailed_data, on='item', how='left')\n",
    "\n",
    "# Save the final data to a CSV file\n",
    "final_data.to_csv('data/wikidata_paintings_final.csv', index=False)\n",
    "print(\"Second pass complete. Detailed painting data saved to wikidata_paintings_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
