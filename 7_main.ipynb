{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import yaml\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse, parse_qs, quote\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load configuration from YAML file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to YAML configuration file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing configuration settings\n",
    "    \"\"\"\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_datetime(series: pd.Series, value_type) -> pd.Series:\n",
    "    \"\"\"Convert series to datetime format.\"\"\"\n",
    "    series = pd.to_datetime(series, errors='coerce')\n",
    "    if value_type == 'year':\n",
    "        return pd.to_numeric(series.dt.year, downcast='integer', errors='coerce')\n",
    "    return series\n",
    "\n",
    "def clean_categorical(series: pd.Series, categories = None) -> pd.Series:\n",
    "    \"\"\"Convert series to categorical format with optional categories.\"\"\"\n",
    "    if categories:\n",
    "        return pd.Categorical(series, categories=categories, ordered=False )\n",
    "    return series.astype('category')\n",
    "\n",
    "\n",
    "def extract_wikidata_id(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Extract the Wikidata ID from a series of URLs.\"\"\"\n",
    "    return series.str.extract(r'(Q\\d+)', expand=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_column(series: pd.Series, dtype: str, value_type: str = None, categories = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Process a single column according to its configuration.\n",
    "    \n",
    "    Args:\n",
    "        series: Column data to process\n",
    "        dtype: Target data type\n",
    "        categories: Optional list of categories for categorical data\n",
    "    \n",
    "    Returns:\n",
    "        Processed column data\n",
    "    \"\"\"\n",
    "    if value_type == 'wikidata_url':\n",
    "        return extract_wikidata_id(series)\n",
    "    \n",
    "    if dtype == 'datetime64[ns]':\n",
    "        return clean_datetime(series, value_type)\n",
    "    elif dtype == 'category':\n",
    "        return clean_categorical(series, categories)\n",
    "    else:\n",
    "        return series.astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_dataset(\n",
    "    source_path: str,\n",
    "    columns_config: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process a single dataset according to configuration.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to source CSV file\n",
    "        columns_config: Configuration for columns\n",
    "        dataset_name: Name of the dataset for specific processing\n",
    "    \n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(source_path)\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mappings = {\n",
    "        config['original_name']: col_name\n",
    "        for col_name, config in columns_config.items()\n",
    "        if 'original_name' in config\n",
    "    }\n",
    "    df = df.rename(columns=column_mappings)\n",
    "    \n",
    "    # Select configured columns\n",
    "    df = df[list(columns_config.keys())]\n",
    "    \n",
    "    # Process each column\n",
    "    for column, config in columns_config.items():\n",
    "        df[column] = process_column(\n",
    "            df[column],\n",
    "            config['dtype'],\n",
    "            config.get('value_type'),\n",
    "            config.get('categories')\n",
    "        )\n",
    "    \n",
    "    # Set index if specified\n",
    "    for column, config in columns_config.items():\n",
    "        if config.get('is_index', False):\n",
    "            df = df.drop_duplicates(subset=column, keep='first')\n",
    "            #df = df.set_index(column)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_all_datasets(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load and process all datasets defined in configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of processed DataFrames\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        dataset_name: load_and_process_dataset(\n",
    "            dataset_config['source'],\n",
    "            dataset_config['columns']\n",
    "        )\n",
    "        for dataset_name, dataset_config in config.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_512px_thumbnail(url):\n",
    "    \"\"\"\n",
    "    Transform a Wikimedia Commons Special:FilePath URL into its 512px thumbnail version.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL in format: http://commons.wikimedia.org/wiki/Special:FilePath/Filename.jpg\n",
    "        \n",
    "    Returns:\n",
    "        str: The 512px thumbnail URL or None if the URL is null\n",
    "    \n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    \n",
    "    # Extract the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    # Construct the 512px thumbnail URL\n",
    "    thumbnail_url = f\"https://commons.wikimedia.org/w/index.php?title=Special:Redirect/file/{filename}&width=512\"\n",
    "    \n",
    "    return thumbnail_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML file\n",
    "config = load_config('config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_all_datasets(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load correspondance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondance_data = all_data['ids']\n",
    "correspondance_data.drop_duplicates(inplace=True)\n",
    "correspondance_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load paintings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paintings_data = all_data['paintings']\n",
    "paintings_data['image_url'] = paintings_data['image_url'].apply(lambda x: get_512px_thumbnail(x) if pd.notna(x) else x)\n",
    "paintings_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load locations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_data = all_data['locations']\n",
    "locations_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load authors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_data = all_data['authors']\n",
    "authors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ML food data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_words = all_data['food_words']\n",
    "food_words['food_word_detected'] = food_words.select_dtypes(include='int').sum(axis=1) > 0\n",
    "food_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_found = all_data['food_found']\n",
    "food_found['food_image_detected'] = food_found['predictions'].apply(lambda x: len(x) > 3)\n",
    "food_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = correspondance_data.merge(paintings_data, on='painting_id', how='inner')\n",
    "merged_df = merged_df.merge(authors_data, on='author_id', how='left')\n",
    "merged_df = merged_df.merge(locations_data, on='location_id', how='left')\n",
    "merged_df = merged_df.merge(food_words, on='painting_id', how='left')\n",
    "merged_df = merged_df.merge(food_found, on='painting_id', how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset='painting_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['food_detected'] = (merged_df['food_word_detected'] | merged_df['food_image_detected']).astype(int)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['image_path'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['food_detected'].value_counts())\n",
    "print(merged_df['food_word_detected'].value_counts())\n",
    "print(merged_df['food_image_detected'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(merged_df[['creation_date','date_of_birth']])\n",
    "display(merged_df[['creation_date','date_of_birth']].describe())\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_year(input_str):\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    # Check if the input string has at least 4 characters and can be converted to an integer\n",
    "    if isinstance(input_str, str) and len(input_str) >= 4:\n",
    "        try:\n",
    "            year = int(input_str[:4])\n",
    "            if year > current_year:\n",
    "                return np.nan\n",
    "            return year\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "merged_df['creation_date'] = merged_df['creation_date'].apply(extract_year)\n",
    "merged_df['date_of_birth'] = merged_df['date_of_birth'].apply(extract_year)\n",
    "\n",
    "\n",
    "display(merged_df[['creation_date','date_of_birth']])\n",
    "display(merged_df[['creation_date','date_of_birth']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing creation year when possible\n",
    "# Calculate the age of the painter at the time of painting\n",
    "merged_df['painter_age_at_painting'] = merged_df['creation_date'] - merged_df['date_of_birth']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(merged_df[['painter', 'creation_date', 'date_of_birth', 'painter_age_at_painting']])\n",
    "\n",
    "# Calculate the average painter_age_at_painting for each painter\n",
    "avg_painter_age = merged_df['painter_age_at_painting'].mean().astype(int)\n",
    "\n",
    "merged_df['painter_age_at_painting'].fillna(avg_painter_age, inplace=True)\n",
    "#merged_df.dropna(subset=['creation_date', 'date_of_birth'], how='all', inplace=True)\n",
    "\n",
    "# Fill missing creation_date with date_of_birth + avg_painter_age\n",
    "merged_df['creation_date'].fillna(merged_df['date_of_birth'] + avg_painter_age, inplace=True)\n",
    "\n",
    "display(merged_df[['painter', 'creation_date', 'date_of_birth', 'painter_age_at_painting']])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['painter'].fillna('Unknown Artist', inplace=True)\n",
    "merged_df['author_country'].fillna('Unknown Country', inplace=True)\n",
    "merged_df['location_country'].fillna('Unknown Country', inplace=True)\n",
    "merged_df['location_name'].fillna('Unknown Location', inplace=True)\n",
    "\n",
    "merged_df['author_gender'] = merged_df['author_gender'].astype('category')\n",
    "merged_df['author_gender'] = merged_df['author_gender'].cat.set_categories(['male', 'female'])\n",
    "merged_df['author_gender'].fillna('male', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a column with decades\n",
    "merged_df['decade'] = (merged_df['creation_date'] // 10) * 10\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(merged_df[['painter', 'creation_date', 'decade']])\n",
    "print(merged_df['decade'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['location_country'] = merged_df['location_country'].replace('German Reich', 'Germany')\n",
    "merged_df['author_country'] = merged_df['author_country'].replace('German Reich', 'Germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_period(decade):\n",
    "    if decade < 1000:\n",
    "        return \"Antiquity\"\n",
    "    elif 1000 <= decade < 1400:\n",
    "        return \"Medieval\"\n",
    "    elif 1400 <= decade < 1500:\n",
    "        return \"Early Renaissance\"\n",
    "    elif 1500 <= decade < 1600:\n",
    "        return \"High Renaissance and Mannerism\"\n",
    "    elif 1600 <= decade < 1700:\n",
    "        return \"Baroque\"\n",
    "    elif 1700 <= decade < 1780:\n",
    "        return \"Rococo\"\n",
    "    elif 1780 <= decade < 1850:\n",
    "        return \"Neoclassicism and Romanticism\"\n",
    "    elif 1850 <= decade < 1900:\n",
    "        return \"Realism and Impressionism\"\n",
    "    elif 1900 <= decade < 1945:\n",
    "        return \"Modern Art\"\n",
    "    elif 1945 <= decade < 1970:\n",
    "        return \"Post-War and Abstract Expressionism\"\n",
    "    elif 1970 <= decade < 2000:\n",
    "        return \"Contemporary Art\"\n",
    "    else:\n",
    "        return \"Contemporary and Digital Art\"\n",
    "\n",
    "\n",
    "\n",
    "merged_df['time_period'] = merged_df['decade'].apply(classify_period)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add gdp and pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_df = pd.read_csv('data/gdp_pop_decades.csv')\n",
    "eco_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(\n",
    "    eco_df,\n",
    "    on='decade',\n",
    "    how='left'  # Keep all artwork records, even if no economic data exists\n",
    ")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize 'gdppc' and 'pop' columns\n",
    "merged_df[['gdppc_normalized', 'pop_normalized']] = scaler.fit_transform(merged_df[['gdppc', 'pop']])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(merged_df[['gdppc', 'gdppc_normalized', 'pop', 'pop_normalized']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export for clip\n",
    "clip_train = merged_df[['painting_id','image_path', 'food_word_detected']]\n",
    "clip_train = clip_train[clip_train['food_word_detected'] == 1]\n",
    "clip_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paintings_with_food = merged_df[merged_df['image_url'].isna() == False]\n",
    "paintings_with_food = paintings_with_food[['title', 'painter', 'creation_date', 'author_gender', 'author_country', 'location_name', 'location_country', 'time_period', 'image_path', 'image_url', 'coordinates','food_detected','decade','gdppc','pop', 'gdppc_normalized', 'pop_normalized']]\n",
    "paintings_with_food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paintings_with_food.to_csv('data/paintings_with_food.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by decade and calculate the proportion of food_detected\n",
    "food_by_decade = merged_df.groupby('decade')['food_detected'].agg(artwork_count='count', food_related_sum='sum').reset_index()\n",
    "food_by_decade['proportion_food_detected'] = food_by_decade['food_related_sum'] / food_by_decade['artwork_count']\n",
    "# Merge normalized GDP and population data\n",
    "food_by_decade = food_by_decade.merge(\n",
    "    merged_df[['decade', 'gdppc_normalized', 'pop_normalized']].drop_duplicates(),\n",
    "    on='decade',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to include only records from 1250 to 2000\n",
    "food_by_decade = food_by_decade[(food_by_decade['decade'] >= 1250) & (food_by_decade['decade'] <= 2000)]\n",
    "# Display the resulting DataFrame\n",
    "food_by_decade.to_csv('data/food_by_decade_analysis.csv', index=False)\n",
    "food_by_decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a DataFrame\n",
    "# Creating the DataFrame directly since we have the data as a string\n",
    "data = food_by_decade\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation = data['proportion_food_detected'].corr(data['gdppc_normalized'])\n",
    "correlation_pvalue = stats.pearsonr(data['proportion_food_detected'], data['gdppc_normalized'])\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = {\n",
    "    'Pearson Correlation': correlation,\n",
    "    'P-value': correlation_pvalue[1],\n",
    "    'Sample Size': len(data),\n",
    "    'Mean Food Proportion': data['proportion_food_detected'].mean(),\n",
    "    'Mean GDP per Capita': data['gdppc_normalized'].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\nCorrelation Analysis Results:\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "\n",
    "# Step 2: Calculate the Pearson correlation coefficient and p-value\n",
    "x = df['gdppc_normalized']\n",
    "y = df['proportion_food_detected']\n",
    "\n",
    "corr_coeff, p_value = pearsonr(x, y)\n",
    "print(f\"Pearson correlation coefficient: {corr_coeff}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "\n",
    "# Step 4: Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant correlation between GDP per capita and the proportion of food artworks.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant correlation between GDP per capita and the proportion of food artworks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
