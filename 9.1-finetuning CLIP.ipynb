{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def setup_model():\n",
    "    \"\"\"Initialize CLIP model with custom classification head\"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    # Freeze CLIP parameters\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Create classification head\n",
    "    classifier = nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 13),\n",
    "        nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    return clip_model, classifier, device\n",
    "\n",
    "def load_and_transform_image(image_path, transform):\n",
    "    \"\"\"Load and transform a single image from local path\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        return transform(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_batch_data(image_paths, labels, transform, device):\n",
    "    \"\"\"Prepare a batch of data\"\"\"\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        img_tensor = load_and_transform_image(img_path, transform)\n",
    "        if img_tensor is not None:\n",
    "            batch_images.append(img_tensor)\n",
    "            batch_labels.append(label)\n",
    "    \n",
    "    if not batch_images:\n",
    "        return None, None\n",
    "    \n",
    "    return (torch.stack(batch_images).to(device), \n",
    "            torch.tensor(batch_labels, dtype=torch.float32).to(device))\n",
    "\n",
    "def train_one_epoch(clip_model, classifier, data_df, food_categories, \n",
    "                   optimizer, criterion, scaler, device, batch_size=32):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Create batches\n",
    "    for start_idx in tqdm(range(0, len(data_df), batch_size)):\n",
    "        batch_df = data_df.iloc[start_idx:start_idx + batch_size]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_images = batch_df['image_path'].tolist()\n",
    "        batch_labels = batch_df[food_categories].values.tolist()\n",
    "        \n",
    "        images, labels = prepare_batch_data(batch_images, batch_labels, transform, device)\n",
    "        if images is None:\n",
    "            continue\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # Get image features from CLIP\n",
    "            with torch.no_grad():\n",
    "                features = clip_model.encode_image(images)\n",
    "            \n",
    "            # Forward pass through classifier\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "def validate(clip_model, classifier, data_df, food_categories, criterion, device, batch_size=32):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, len(data_df), batch_size):\n",
    "            batch_df = data_df.iloc[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            batch_images = batch_df['image_path'].tolist()\n",
    "            batch_labels = batch_df[food_categories].values.tolist()\n",
    "            \n",
    "            images, labels = prepare_batch_data(batch_images, batch_labels, transform, device)\n",
    "            if images is None:\n",
    "                continue\n",
    "                \n",
    "            features = clip_model.encode_image(images)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "def main():\n",
    "    # Define food categories\n",
    "    food_categories = ['fruit', 'bread', 'cookware', 'seafood', 'wine', \n",
    "                      'meal', 'cheese', 'meat', 'food', 'beverage', \n",
    "                      'dairy', 'vegetable', 'dessert']\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv('your_data.csv')  # Should contain 'image_path' column\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize models and optimizer\n",
    "    clip_model, classifier, device = setup_model()\n",
    "    optimizer = AdamW(classifier.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        classifier.train()\n",
    "        train_loss = train_one_epoch(clip_model, classifier, train_df, food_categories,\n",
    "                                   optimizer, criterion, scaler, device)\n",
    "        \n",
    "        # Validate\n",
    "        classifier.eval()\n",
    "        val_loss = validate(clip_model, classifier, val_df, food_categories,\n",
    "                          criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'classifier_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': val_loss\n",
    "            }, 'best_model.pth')\n",
    "\n",
    "def predict(image_path, clip_model, classifier, device):\n",
    "    \"\"\"Make prediction for a single image\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = load_and_transform_image(image_path, transform)\n",
    "    if img_tensor is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.encode_image(img_tensor.unsqueeze(0).to(device))\n",
    "        outputs = classifier(features)\n",
    "        \n",
    "    return outputs.cpu().numpy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
