{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the IDs from the SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run the SPARQL query\n",
    "def run_sparql_query(query):\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    # Set your user agent to comply with Wikidata's policy\n",
    "    sparql.addCustomHttpHeader('User-Agent', 'MyPaintingDataRetriever/1.0 (jipijipijipi@gmail.com)')\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "# Function to chunk the list into batches\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all the data from original list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/wikidata_authors.csv'):\n",
    "    print(\"Final file already exists. Skipping data retrieval.\")\n",
    "else:\n",
    "    # Define the SPARQL endpoint URL\n",
    "    wikidata_endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    # Read the basic painting data\n",
    "    basic_data = pd.read_csv('data/wikidata_paintings_ids.csv')\n",
    "    authors = basic_data[['author_wikidata', 'author_name']].drop_duplicates(\n",
    "        subset=['author_wikidata']).reset_index(drop=True)\n",
    "    authors_list = authors['author_wikidata'].tolist()\n",
    "\n",
    "    # Set batch parameters\n",
    "    batch_size = 50  # Number of paintings to query at once\n",
    "    max_retries = 5  # Maximum number of retries for failed requests\n",
    "\n",
    "    # Prepare to store detailed data\n",
    "    detailed_data = pd.DataFrame()\n",
    "\n",
    "    # Check if a checkpoint exists to resume from\n",
    "    if os.path.exists('data/checkpoints/authors_checkpoint.csv') and os.path.exists('data/checkpoints/authors_batch_index_checkpoint.txt'):\n",
    "        detailed_data = pd.read_csv('data/checkpoints/authors_checkpoint.csv')\n",
    "        with open('data/checkpoints/authors_batch_index_checkpoint.txt', 'r') as f:\n",
    "            start_batch = int(f.read())\n",
    "        print(f\"Resuming from batch index {start_batch}\")\n",
    "    else:\n",
    "        detailed_data = pd.DataFrame()\n",
    "        start_batch = 0\n",
    "\n",
    "    # Convert item URIs to Q-IDs\n",
    "    author_qids = [uri.split('/')[-1] for uri in authors_list]\n",
    "\n",
    "    # Create batches\n",
    "    batches = list(chunk_list(author_qids, batch_size))\n",
    "\n",
    "    # Loop over batches to fetch detailed data\n",
    "    for batch_index, batch_qids in enumerate(batches[start_batch:], start=start_batch):\n",
    "        print(f\"Processing batch {batch_index + 1}/{len(batches)}\")\n",
    "        qid_list_str = ' '.join(f'wd:{qid}' for qid in batch_qids)\n",
    "\n",
    "        # Construct the SPARQL query for the batch\n",
    "        batch_query = f\"\"\"\n",
    "                        SELECT ?author ?author_name ?country ?country_label ?gender ?gender_label ?date_of_birth \n",
    "                            ?place_of_birth ?place_of_birth_label ?place_of_birth_country ?place_of_birth_country_label WHERE {{\n",
    "                        VALUES ?author {{ {qid_list_str} }}\n",
    "                        \n",
    "                        OPTIONAL {{\n",
    "                            ?author wdt:P27 ?country.                             # P27 = country of citizenship\n",
    "                            ?country rdfs:label ?country_label.                   # Get the label for country of citizenship\n",
    "                            FILTER(LANG(?country_label) = \"en\")\n",
    "                        }}\n",
    "                        \n",
    "                        OPTIONAL {{\n",
    "                            ?author wdt:P21 ?gender.                              # P21 = gender\n",
    "                            ?gender rdfs:label ?gender_label.                     # Get the label for gender\n",
    "                            FILTER(LANG(?gender_label) = \"en\")\n",
    "                        }}\n",
    "                        \n",
    "                        OPTIONAL {{\n",
    "                            ?author wdt:P569 ?date_of_birth.                      # P569 = date of birth\n",
    "                        }}\n",
    "                        \n",
    "                        OPTIONAL {{\n",
    "                            ?author wdt:P19 ?place_of_birth.                      # P19 = place of birth\n",
    "                            ?place_of_birth rdfs:label ?place_of_birth_label.     # Get the label for place of birth\n",
    "                            FILTER(LANG(?place_of_birth_label) = \"en\")\n",
    "                            \n",
    "                            OPTIONAL {{\n",
    "                            ?place_of_birth wdt:P17 ?place_of_birth_country.      # P17 = country for the place of birth\n",
    "                            ?place_of_birth_country rdfs:label ?place_of_birth_country_label.  # Get the label for this country\n",
    "                            FILTER(LANG(?place_of_birth_country_label) = \"en\")\n",
    "                            }}\n",
    "                        }}\n",
    "                        \n",
    "                        OPTIONAL {{\n",
    "                            ?author rdfs:label ?author_name.                      # Get the author's name\n",
    "                            FILTER(LANG(?author_name) = \"en\")\n",
    "                        }}\n",
    "                        }}\n",
    "                        \"\"\"\n",
    "\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                results = run_sparql_query(batch_query)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "                retries += 1\n",
    "                time.sleep(5)\n",
    "        else:\n",
    "            print(\"Max retries exceeded for this batch. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Process the results\n",
    "        bindings = results['results']['bindings']\n",
    "        if not bindings:\n",
    "            print(f\"No data returned for batch {batch_index + 1}.\")\n",
    "            continue\n",
    "\n",
    "        # Convert the bindings to a DataFrame\n",
    "        data = []\n",
    "        for b in bindings:\n",
    "            author = b['author']['value']\n",
    "            author_name = b.get('author_name', {}).get('value', None)\n",
    "            country = b.get('country_label', {}).get('value', None)\n",
    "            birth_country = b.get('place_of_birth_country_label', {}).get('value', None)\n",
    "            birth_place = b.get('place_of_birth_label', {}).get('value', None)\n",
    "            gender = b.get('gender_label', {}).get('value', None)\n",
    "            date_of_birth = b.get('date_of_birth', {}).get('value', None)\n",
    "\n",
    "            data.append({\n",
    "                'author_wikidata': author,\n",
    "                'name': author_name,\n",
    "                'country': country,\n",
    "                'birth_country': birth_country,\n",
    "                'birth_place': birth_place,\n",
    "                'gender': gender,\n",
    "                'date_of_birth': date_of_birth,\n",
    "            })\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        detailed_data = pd.concat([detailed_data, df], ignore_index=True)\n",
    "\n",
    "        # Save a checkpoint\n",
    "        detailed_data.to_csv(\n",
    "            'data/checkpoints/authors_checkpoint.csv', index=False)\n",
    "        with open('data/checkpoints/authors_batch_index_checkpoint.txt', 'w') as f:\n",
    "            f.write(str(batch_index + 1))\n",
    "        print(f\"Checkpoint saved at batch {batch_index + 1}\")\n",
    "\n",
    "        time.sleep(1)  # Be polite and avoid overloading the server\n",
    "\n",
    "    # Merge basic and detailed data\n",
    "    detailed_data.drop_duplicates(subset=['author_wikidata'], inplace=True)\n",
    "    final_data = pd.merge(authors, detailed_data, on='author_wikidata', how='left')\n",
    "    # Save the final data to a CSV file\n",
    "    final_data.to_csv('data/wikidata_authors.csv', index=False)\n",
    "    print(\"Second pass complete. Detailed painting data saved to wikidata_authors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
