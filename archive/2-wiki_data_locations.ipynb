{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the IDs from the SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run the SPARQL query\n",
    "def run_sparql_query(query):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    # Set your user agent to comply with Wikidata's policy\n",
    "    sparql.addCustomHttpHeader('User-Agent', 'MyPaintingDataRetriever/1.0 (jipijipijipi@gmail.com)')\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "# Function to chunk the list into batches\n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all the data from original list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SPARQL endpoint URL\n",
    "wikidata_endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# Read the basic painting data\n",
    "basic_data = pd.read_csv('data/wikidata_paintings_ids_final.csv')\n",
    "basic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations = basic_data['location_wikidata']\n",
    "locations_list = locations.unique().tolist()\n",
    "\n",
    "# Set batch parameters\n",
    "batch_size = 50  # Number of locations to query at once\n",
    "max_retries = 5  # Maximum number of retries for failed requests\n",
    "\n",
    "# Prepare to store detailed data\n",
    "detailed_data = pd.DataFrame()\n",
    "\n",
    "# Check if a checkpoint exists to resume from\n",
    "if os.path.exists('data/checkpoints/locations_checkpoint.csv') and os.path.exists('data/checkpoints/locations_batch_index_checkpoint.txt'):\n",
    "    detailed_data = pd.read_csv('data/checkpoints/locations_checkpoint.csv')\n",
    "    with open('data/checkpoints/locations_batch_index_checkpoint.txt', 'r') as f:\n",
    "        start_batch = int(f.read())\n",
    "    print(f\"Resuming from batch index {start_batch}\")\n",
    "else:\n",
    "    detailed_data = pd.DataFrame()\n",
    "    start_batch = 0\n",
    "\n",
    "# Convert item URIs to Q-IDs\n",
    "item_qids = [uri.split('/')[-1] for uri in locations_list]\n",
    "\n",
    "# Create batches\n",
    "batches = list(chunk_list(item_qids, batch_size))\n",
    "\n",
    "# Loop over batches to fetch detailed data\n",
    "for batch_index, batch_qids in enumerate(batches[start_batch:], start=start_batch):\n",
    "    print(f\"Processing batch {batch_index + 1}/{len(batches)}\")\n",
    "    qid_list_str = ' '.join(f'wd:{qid}' for qid in batch_qids)\n",
    "\n",
    "    # Construct the SPARQL query for the batch\n",
    "    batch_query = f\"\"\"\n",
    "                    SELECT ?museum ?museum_name ?city ?city_label ?country ?country_label ?founding_date ?museum_type ?museum_type_label ?coordinates ?part_of WHERE  {{\n",
    "                    VALUES ?museum {{ {qid_list_str} }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P17 ?country.                           \n",
    "                        ?country rdfs:label ?country_label.              \n",
    "                        FILTER(LANG(?country_label) = \"en\")\n",
    "                    }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P131 ?city.                     \n",
    "                        ?city rdfs:label ?city_label.                \n",
    "                        FILTER(LANG(?city_label) = \"en\")\n",
    "                    }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P571 ?founding_date.                    \n",
    "                    }}\n",
    "                    \n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P31 ?museum_type.                        \n",
    "                        ?museum_type rdfs:label ?museum_type_label.           \n",
    "                        FILTER(LANG(?museum_type_label) = \"en\")\n",
    "                    }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P625 ?coordinates.                       \n",
    "                    }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum rdfs:label ?museum_name.                     \n",
    "                        FILTER(LANG(?museum_name) = \"en\")\n",
    "                    }}\n",
    "                    \n",
    "                    OPTIONAL {{\n",
    "                        ?museum wdt:P361 ?part_of.                            \n",
    "                    }}\n",
    "                    }}\n",
    "                    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            results = run_sparql_query(batch_query)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print(\"Max retries exceeded for this batch. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Process the results\n",
    "    bindings = results['results']['bindings']\n",
    "    if not bindings:\n",
    "        print(f\"No data returned for batch {batch_index + 1}.\")\n",
    "        continue\n",
    "\n",
    "    # Convert the bindings to a DataFrame\n",
    "    data = []\n",
    "    for b in bindings:\n",
    "        location_wikidata = b['museum']['value']\n",
    "        location_name = b.get('museum_name', {}).get('value', None)\n",
    "        country = b.get('country_label', {}).get('value', None)\n",
    "        founding_date = b.get('founding_date', {}).get('value', None)\n",
    "        collection_size = b.get('collection_size', {}).get('value', None)\n",
    "        museum_type = b.get('museum_type_label', {}).get('value', None)\n",
    "        coordinates = b.get('coordinates', {}).get('value', None)\n",
    "        part_of = b.get('part_of', {}).get('value', None)\n",
    "\n",
    "        data.append({\n",
    "            'location_wikidata': location_wikidata,\n",
    "            'name': location_name,\n",
    "            'country': country,\n",
    "            'founding_date': founding_date,\n",
    "            'collection_size': collection_size,\n",
    "            'museum_type': museum_type,\n",
    "            'coordinates': coordinates,\n",
    "            'part_of': part_of\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    detailed_data = pd.concat([detailed_data, df], ignore_index=True)\n",
    "\n",
    "    # Save a checkpoint\n",
    "    detailed_data.to_csv(\n",
    "        'data/checkpoints/locations_checkpoint.csv', index=False)\n",
    "    with open('data/checkpoints/locations_batch_index_checkpoint.txt', 'w') as f:\n",
    "        f.write(str(batch_index + 1))\n",
    "    print(f\"Checkpoint saved at batch {batch_index + 1}\")\n",
    "\n",
    "    time.sleep(1)  # Be polite and avoid overloading the server\n",
    "\n",
    "# Merge basic and detailed data\n",
    "detailed_data.drop_duplicates(subset=['location_wikidata'], inplace=True)\n",
    "final_data = pd.merge(locations, detailed_data, on='location_wikidata', how='left')\n",
    "\n",
    "# Save the final data to a CSV file\n",
    "final_data.to_csv('data/wikidata_locations.csv', index=False)\n",
    "print(\"Location retrieval complete. Detailed painting data saved to wikidata_locations.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
