{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>image_path</th>\n",
       "      <th>has_food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q27064304</td>\n",
       "      <td>img/img_512/Intérieur de cuisine - Joachim Beu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q12900365</td>\n",
       "      <td>img/img_512/The Luncheon (SM sg170).png</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q776175</td>\n",
       "      <td>img/img_512/Pieter Bruegel the Elder- The Harv...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q72701665</td>\n",
       "      <td>img/img_512/Lille PdBA quellin fyt jesus marth...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q20532659</td>\n",
       "      <td>img/img_512/OA Hermansen, Et frokostbord, 1884...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71165</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51247485</td>\n",
       "      <td>img/img_512/Alex Colville - Infantry, near Nij...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71166</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51244389</td>\n",
       "      <td>img/img_512/Ivan Žabota - dekliški portret.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71167</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51235353</td>\n",
       "      <td>img/img_512/Ivan Žabota - Marta Krásovej.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71168</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51265369</td>\n",
       "      <td>img/img_512/Ivan Žabota - ženski portret.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71169</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51246257</td>\n",
       "      <td>img/img_512/Ľudovít Pitthordt - Vlastná podobi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71170 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           item  \\\n",
       "0      http://www.wikidata.org/entity/Q27064304   \n",
       "1      http://www.wikidata.org/entity/Q12900365   \n",
       "2        http://www.wikidata.org/entity/Q776175   \n",
       "3      http://www.wikidata.org/entity/Q72701665   \n",
       "4      http://www.wikidata.org/entity/Q20532659   \n",
       "...                                         ...   \n",
       "71165  http://www.wikidata.org/entity/Q51247485   \n",
       "71166  http://www.wikidata.org/entity/Q51244389   \n",
       "71167  http://www.wikidata.org/entity/Q51235353   \n",
       "71168  http://www.wikidata.org/entity/Q51265369   \n",
       "71169  http://www.wikidata.org/entity/Q51246257   \n",
       "\n",
       "                                              image_path  has_food  \n",
       "0      img/img_512/Intérieur de cuisine - Joachim Beu...      True  \n",
       "1                img/img_512/The Luncheon (SM sg170).png      True  \n",
       "2      img/img_512/Pieter Bruegel the Elder- The Harv...      True  \n",
       "3      img/img_512/Lille PdBA quellin fyt jesus marth...      True  \n",
       "4      img/img_512/OA Hermansen, Et frokostbord, 1884...      True  \n",
       "...                                                  ...       ...  \n",
       "71165  img/img_512/Alex Colville - Infantry, near Nij...     False  \n",
       "71166     img/img_512/Ivan Žabota - dekliški portret.jpg     False  \n",
       "71167       img/img_512/Ivan Žabota - Marta Krásovej.jpg     False  \n",
       "71168       img/img_512/Ivan Žabota - ženski portret.jpg     False  \n",
       "71169  img/img_512/Ľudovít Pitthordt - Vlastná podobi...     False  \n",
       "\n",
       "[71170 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/paintings_with_food_nlp.csv')\n",
    "df['has_food'] = df.iloc[:, 2:].sum(axis=1) > 0\n",
    "df = df[['item', 'image_path', 'has_food']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 71170\n",
      "With food: 1407\n",
      "Without food: 69763\n",
      "Ratio food/no-food: 0.01976956582829844\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming your dataframe is called df\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"With food:\", sum(df['has_food']))\n",
    "print(\"Without food:\", sum(~df['has_food']))\n",
    "print(\"Ratio food/no-food:\", sum(df['has_food'])/len(df))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_input(image_path, has_food, processor):\n",
    "    \"\"\"Process a single image and create corresponding text\"\"\"\n",
    "    try:\n",
    "        # Open image and ensure it's RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Resize image to CLIP's expected size (224x224)\n",
    "        image = TF.resize(image, (224, 224), interpolation=TF.InterpolationMode.BICUBIC)\n",
    "        image = TF.center_crop(image, (224, 224))\n",
    "        \n",
    "        text = 'a painting containing food' if has_food else 'a painting not containing food'\n",
    "        \n",
    "        # Process using CLIP processor\n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=[text],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Add label (squeeze to make it a scalar)\n",
    "        inputs['labels'] = torch.tensor([float(has_food)])\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_batch(samples):\n",
    "    \"\"\"Collate function to create batches, handling None values\"\"\"\n",
    "    # Filter out None values\n",
    "    samples = [s for s in samples if s is not None]\n",
    "    \n",
    "    if not samples:\n",
    "        return None\n",
    "        \n",
    "    batch = {\n",
    "        'pixel_values': torch.stack([x['pixel_values'][0] for x in samples]),\n",
    "        'input_ids': torch.stack([x['input_ids'][0] for x in samples]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'][0] for x in samples]),\n",
    "        'labels': torch.stack([x['labels'] for x in samples])\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "            \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            pixel_values=batch['pixel_values']\n",
    "        )\n",
    "        \n",
    "        # Get image and text features\n",
    "        image_features = outputs.image_embeds\n",
    "        text_features = outputs.text_embeds\n",
    "        \n",
    "        # Compute similarity scores (ensure it's the right shape)\n",
    "        similarity = torch.sum(image_features * text_features, dim=-1)  # [batch_size]\n",
    "        \n",
    "        # Compute loss (make sure both tensors are the same shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            similarity,  # [batch_size]\n",
    "            batch['labels'].float().squeeze(),  # [batch_size]\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                pixel_values=batch['pixel_values']\n",
    "            )\n",
    "            \n",
    "            image_features = outputs.image_embeds\n",
    "            text_features = outputs.text_embeds\n",
    "            similarity = torch.sum(image_features * text_features, dim=-1)\n",
    "            \n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                similarity,\n",
    "                batch['labels'].float().squeeze()\n",
    "            )\n",
    "            \n",
    "            predictions = (torch.sigmoid(similarity) > 0.5).float()\n",
    "            correct += (predictions == batch['labels'].squeeze()).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / len(eval_loader) if len(eval_loader) > 0 else float('inf')\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_model(df, num_epochs=3, batch_size=16, learning_rate=5e-5):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    model_name = \"openai/clip-vit-base-patch32\"\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Balance dataset\n",
    "    food_samples = df[df['has_food']]\n",
    "    no_food_samples = df[~df['has_food']]\n",
    "    min_samples = min(len(food_samples), len(no_food_samples))\n",
    "    \n",
    "    # Undersample majority class\n",
    "    if len(food_samples) > len(no_food_samples):\n",
    "        food_samples = food_samples.sample(n=min_samples, random_state=42)\n",
    "    else:\n",
    "        no_food_samples = no_food_samples.sample(n=min_samples, random_state=42)\n",
    "    \n",
    "    balanced_df = pd.concat([food_samples, no_food_samples])\n",
    "    print(f\"Balanced dataset size: {len(balanced_df)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Process datasets with error handling\n",
    "    train_samples = []\n",
    "    for _, row in tqdm(train_df.iterrows(), desc=\"Processing train data\"):\n",
    "        sample = prepare_input(row['image_path'], row['has_food'], processor)\n",
    "        if sample is not None:\n",
    "            train_samples.append(sample)\n",
    "    \n",
    "    val_samples = []\n",
    "    for _, row in tqdm(val_df.iterrows(), desc=\"Processing val data\"):\n",
    "        sample = prepare_input(row['image_path'], row['has_food'], processor)\n",
    "        if sample is not None:\n",
    "            val_samples.append(sample)\n",
    "    \n",
    "    print(f\"Processed {len(train_samples)} training samples\")\n",
    "    print(f\"Processed {len(val_samples)} validation samples\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_samples,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=create_batch\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_samples,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=create_batch\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, accuracy = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save if best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': accuracy,\n",
    "            }, 'best_food_detector.pth')\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def predict(image_path, model, processor, device):\n",
    "    \"\"\"Make prediction for a single image\"\"\"\n",
    "    inputs = prepare_input(image_path, False, processor)  # label doesn't matter here\n",
    "    if inputs is None:\n",
    "        return None, None\n",
    "        \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            pixel_values=inputs['pixel_values']\n",
    "        )\n",
    "        \n",
    "        image_features = outputs.image_embeds\n",
    "        text_features = outputs.text_embeds\n",
    "        similarity = torch.sum(image_features * text_features, dim=-1)\n",
    "        \n",
    "    probability = torch.sigmoid(similarity).cpu().numpy()[0]\n",
    "    return probability > 0.5, probability\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_balanced_data(df, balance_strategy='undersample'):\n",
    "    \"\"\"\n",
    "    Balance the dataset using different strategies\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with 'has_food' column\n",
    "    - balance_strategy: 'undersample', 'oversample', or 'weighted'\n",
    "    \n",
    "    Returns:\n",
    "    - Balanced DataFrame or (DataFrame, sample_weights)\n",
    "    \"\"\"\n",
    "    food_samples = df[df['has_food']]\n",
    "    no_food_samples = df[~df['has_food']]\n",
    "    \n",
    "    print(f\"Original distribution:\")\n",
    "    print(f\"Food samples: {len(food_samples)}\")\n",
    "    print(f\"No food samples: {len(no_food_samples)}\")\n",
    "    \n",
    "    if balance_strategy == 'undersample':\n",
    "        # Undersample majority class\n",
    "        no_food_balanced = resample(\n",
    "            no_food_samples,\n",
    "            replace=False,\n",
    "            n_samples=len(food_samples),\n",
    "            random_state=42\n",
    "        )\n",
    "        balanced_df = pd.concat([food_samples, no_food_balanced])\n",
    "        print(f\"\\nAfter undersampling:\")\n",
    "        print(f\"Total samples: {len(balanced_df)}\")\n",
    "        return balanced_df\n",
    "        \n",
    "    elif balance_strategy == 'oversample':\n",
    "        # Oversample minority class\n",
    "        food_balanced = resample(\n",
    "            food_samples,\n",
    "            replace=True,\n",
    "            n_samples=len(no_food_samples),\n",
    "            random_state=42\n",
    "        )\n",
    "        balanced_df = pd.concat([food_balanced, no_food_samples])\n",
    "        print(f\"\\nAfter oversampling:\")\n",
    "        print(f\"Total samples: {len(balanced_df)}\")\n",
    "        return balanced_df\n",
    "        \n",
    "    elif balance_strategy == 'weighted':\n",
    "        # Calculate class weights\n",
    "        total_samples = len(df)\n",
    "        weight_for_0 = (1 / len(no_food_samples)) * (total_samples / 2)\n",
    "        weight_for_1 = (1 / len(food_samples)) * (total_samples / 2)\n",
    "        \n",
    "        sample_weights = np.where(df['has_food'], weight_for_1, weight_for_0)\n",
    "        print(\"\\nUsing weighted sampling\")\n",
    "        print(f\"Weight for no food: {weight_for_0:.3f}\")\n",
    "        print(f\"Weight for food: {weight_for_1:.3f}\")\n",
    "        return df, sample_weights\n",
    "\n",
    "def modify_train_function_for_weights(train_model_fn):\n",
    "    \"\"\"\n",
    "    Modify the training function to use sample weights\n",
    "    \"\"\"\n",
    "    def weighted_loss(predictions, targets, weights):\n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            predictions, \n",
    "            targets,\n",
    "            weight=weights,\n",
    "            reduction='mean'\n",
    "        )\n",
    "    \n",
    "    # Modify the training loop to include weights\n",
    "    def train_epoch_weighted(model, train_loader, optimizer, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                pixel_values=batch['pixel_values']\n",
    "            )\n",
    "            \n",
    "            image_features = outputs.image_embeds\n",
    "            text_features = outputs.text_embeds\n",
    "            similarity = torch.sum(image_features * text_features, dim=-1)\n",
    "            \n",
    "            loss = weighted_loss(\n",
    "                similarity, \n",
    "                batch['labels'],\n",
    "                batch['weights'].to(device)\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    return train_epoch_weighted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      "Food samples: 1407\n",
      "No food samples: 69763\n",
      "\n",
      "After undersampling:\n",
      "Total samples: 2814\n",
      "Using device: mps\n",
      "Balanced dataset size: 2814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e34f04d4fa54814958983c98bb97aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fce41e5d9444f59696713fded54fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2251 training samples\n",
      "Processed 563 validation samples\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43147fc2e83e4d2e96fd58ef48dced03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3272\n",
      "Val Loss: 0.3133\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49d737d4944fc9a066c4ca66074572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3133\n",
      "Val Loss: 0.3133\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c75412a14142c881162b5951655d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3133\n",
      "Val Loss: 0.3133\n",
      "Accuracy: 1.0000\n",
      "Contains food: False (confidence: 0.27)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Choose one of these approaches:\n",
    "\n",
    "# 1. Undersampling approach\n",
    "balanced_df = prepare_balanced_data(df, 'undersample')\n",
    "\n",
    "\"\"\"  # 2. Oversampling approach\n",
    "balanced_df = prepare_balanced_data(df, 'oversample')\n",
    "model, processor = train_model(balanced_df)\n",
    "\n",
    "# 3. Weighted approach\n",
    "df, sample_weights = prepare_balanced_data(df, 'weighted')\n",
    "# You'll need to modify the train_model function to use weights \"\"\"\n",
    "\n",
    "# Train model\n",
    "# Verify image paths exist and are accessible\n",
    "#balanced_df['exists'] = balanced_df['image_path'].apply(lambda x: Path(x).exists())\n",
    "#print(f\"Found {sum(df['exists'])} valid images out of {len(df)} total\")\n",
    "\n",
    "# Filter to only existing images\n",
    "#balanced_df = balanced_df[balanced_df['exists']]\n",
    "\n",
    "# Train model\n",
    "model, processor = train_model(balanced_df)\n",
    "\n",
    "# Example prediction\n",
    "    # Test prediction\n",
    "test_image = df['image_path'].iloc[0]\n",
    "contains_food, confidence = predict(test_image, model, processor, \n",
    "                                    torch.device('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Contains food: {contains_food} (confidence: {confidence:.2f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img/img_512/Intérieur de cuisine - Joachim Beuckelaer - Musée du louvre Peintures RF 2659.jpg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
