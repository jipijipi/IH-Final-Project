{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food in Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the IDs from the SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run the SPARQL query\n",
    "def run_sparql_query(query):\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    # Set your user agent to comply with Wikidata's policy\n",
    "    sparql.addCustomHttpHeader('User-Agent', 'MyPaintingDataRetriever/1.0 (jipijipijipi@gmail.com)')\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "# Function to chunk the list into batches\n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_endpoint_url = \"https://query.wikidata.org/sparql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_base_query = \"\"\"\n",
    "SELECT ?item ?author_wikidata ?location_wikidata WHERE {{\n",
    "?item wdt:P31 wd:Q3305213.\n",
    "?item wdt:P170 ?author_wikidata.\n",
    "OPTIONAL {{ ?item wdt:P276 ?location_wikidata. }}\n",
    "}}\n",
    "LIMIT {limit}\n",
    "OFFSET {offset}\n",
    "\"\"\"\n",
    "\n",
    "# Set batch parameters\n",
    "limit = 1000  # Number of records to fetch per batch\n",
    "checkpoint_interval = 10  # Save a checkpoint every 10 batches\n",
    "max_retries = 5  # Maximum number of retries for failed requests\n",
    "\n",
    "# Check if a checkpoint exists to resume from\n",
    "# Create checkpoints folder if it does not exist\n",
    "if not os.path.exists('data/checkpoints'):\n",
    "    os.makedirs('data/checkpoints')\n",
    "    \n",
    "if os.path.exists('data/checkpoints/paintings_ids_checkpoint.csv') and os.path.exists('data/checkpoints/offset_paintings_ids_checkpoint.txt'):\n",
    "    all_data = pd.read_csv('data/checkpoints/paintings_ids_checkpoint.csv')\n",
    "    with open('data/checkpoints/offset_paintings_ids_checkpoint.txt', 'r') as f:\n",
    "        offset = int(f.read())\n",
    "    batch_number = offset // limit\n",
    "    print(f\"Resuming from offset {offset}\")\n",
    "else:\n",
    "    all_data = pd.DataFrame()\n",
    "    offset = 0\n",
    "    batch_number = 0\n",
    "\n",
    "# Loop to fetch data in batches\n",
    "\n",
    "while True:\n",
    "    query = wikidata_base_query.format(limit=limit, offset=offset)\n",
    "    print(query)\n",
    "    print(f\"Fetching data with OFFSET {offset}\")\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            results = run_sparql_query(query)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Retrying ({retries+1}/{max_retries})...\")\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print(\"Max retries exceeded. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # Check if the results object is empty or None\n",
    "    if not results or 'results' not in results or 'bindings' not in results['results'] or not results['results']['bindings']:\n",
    "        print(\"No more data returned.\")\n",
    "        break\n",
    "    \n",
    "    # Process the results\n",
    "    bindings = results['results']['bindings']\n",
    "    if not bindings:\n",
    "        print(\"No more data returned.\")\n",
    "        break\n",
    "\n",
    "    # Convert the bindings to a DataFrame\n",
    "    data = []\n",
    "    for b in bindings:\n",
    "        item = b['item']['value']\n",
    "        author_wikidata = b['author_wikidata']['value']\n",
    "        location_wikidata = b['location_wikidata']['value']\n",
    "        data.append({\n",
    "            'item': item,\n",
    "            'author_wikidata': author_wikidata,\n",
    "            'location_wikidata': location_wikidata\n",
    "        })\n",
    "    df = pd.DataFrame(data)\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Save a checkpoint at specified intervals\n",
    "    batch_number += 1\n",
    "    if batch_number % checkpoint_interval == 0:\n",
    "        all_data.to_csv('data/checkpoints/paintings_ids_checkpoint.csv', index=False)\n",
    "        with open('data/checkpoints/offset_paintings_ids_checkpoint.txt', 'w') as f:\n",
    "            f.write(str(offset + limit))\n",
    "        print(f\"Checkpoint saved at batch {batch_number}\")\n",
    "\n",
    "    # Update the offset for the next batch\n",
    "    offset += limit\n",
    "    time.sleep(1)  # Be polite and avoid overloading the server\n",
    "\n",
    "# Save the final data to a CSV file\n",
    "unique_paintings = all_data.drop_duplicates(subset='item', keep='first')\n",
    "unique_paintings.to_csv('data/wikidata_paintings_ids_final.csv', index=False)\n",
    "print(\"Data retrieval complete. Saved to wikidata_paintings_ids_final.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
